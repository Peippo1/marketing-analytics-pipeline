# ğŸ“Š Marketing Analytics Pipeline
A modular ETL pipeline for marketing data built with PySpark and Delta Lake.

This project demonstrates a full end-to-end data engineering workflow for a marketing analytics use case, powered by PySpark, Delta Lake, and the Databricks platform.

We use customer and campaign data (sourced from Kaggle) to extract insights, perform customer segmentation, and explore predictive modellingâ€”all while showcasing best practices in data transformation and Lakehouse architecture.

## ğŸš€ Project Goals

- Build a scalable ETL pipeline using PySpark and Delta Lake
- Clean, transform, and enrich marketing data
- Generate key KPIs: campaign response rates, customer value, and segment profiles
- Experiment with machine learning models for lead scoring and segmentation
- Visualise insights using Streamlit or Power BI

## ğŸ§± Tech Stack

- **Language**: Python
- **Data Processing**: PySpark, Delta Lake
- **ML/AI**: scikit-learn, MLflow
- **Scheduling** (optional): Airflow or Databricks Workflows
- **Dashboarding**: Streamlit / Power BI
- **Data Source**: [Kaggle Marketing Data](https://www.kaggle.com/datasets/jackdaoud/marketing-data)

## ğŸ“ Project Structure

```
marketing-analytics-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw CSVs from Kaggle
â”‚   â””â”€â”€ processed/        # Delta output
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda.ipynb      # Exploratory Data Analysis with visual insights
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ extract.py        # Load CSV into Spark
â”‚   â”œâ”€â”€ transform.py      # Clean and engineer features
â”‚   â””â”€â”€ load.py           # Save to Delta Lake
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lead_scoring_model.pkl
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ mlflow/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§  Model Pipeline Features

- Logistic Regression model with scikit-learn
- 5-fold cross-validation for performance estimation
- Hyperparameter tuning via GridSearchCV
- Versioned model saving (timestamped .pkl files)
- Evaluation metrics saved to JSON for monitoring

## ğŸ”œ Next Steps

- [x] Complete ETL pipeline and convert to reusable scripts
- [x] Run exploratory analysis on customer behavior
- [x] Build segmentation or response model
- [ ] Deploy dashboard for insights
- [ ] Add unit tests for pipeline components
- [ ] Schedule daily pipeline using Airflow
- [ ] Containerize with Docker for local + cloud execution
